{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='../.env', # If None the function will try to find in the working directory\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-3.5-turbo\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from typing import List, Dict\n",
    "\n",
    "# class CustomGroupChat(GroupChat):\n",
    "#     def __init__(self, agents, messages, max_round=10):\n",
    "#         super().__init__(agents, messages, max_round)\n",
    "#         self.previous_speaker = None  # Keep track of the previous speaker\n",
    "    \n",
    "#     def select_speaker(self, last_speaker: Agent, selector: AssistantAgent):\n",
    "#         # Check if last message suggests a next speaker or termination\n",
    "#         last_message = self.messages[-1] if self.messages else None\n",
    "#         if last_message:\n",
    "#             if 'NEXT:' in last_message['content']:\n",
    "#                 suggested_next = last_message['content'].split('NEXT: ')[-1].strip()\n",
    "#                 print(f'Extracted suggested_next = {suggested_next}')\n",
    "#                 try:\n",
    "#                     return self.agent_by_name(suggested_next)\n",
    "#                 except ValueError:\n",
    "#                     pass  # If agent name is not valid, continue with normal selection\n",
    "#             elif 'TERMINATE' in last_message['content']:\n",
    "#                 try:\n",
    "#                     return self.agent_by_name('User_proxy')\n",
    "#                 except ValueError:\n",
    "#                     pass  # If 'User_proxy' is not a valid name, continue with normal selection\n",
    "        \n",
    "#         team_leader_names = [agent.name for agent in self.agents if agent.name.endswith('1')]\n",
    "\n",
    "#         if last_speaker.name in team_leader_names:\n",
    "#             team_letter = last_speaker.name[0]\n",
    "#             possible_next_speakers = [\n",
    "#                 agent for agent in self.agents if (agent.name.startswith(team_letter) or agent.name in team_leader_names) \n",
    "#                 and agent != last_speaker and agent != self.previous_speaker\n",
    "#             ]\n",
    "#         else:\n",
    "#             team_letter = last_speaker.name[0]\n",
    "#             possible_next_speakers = [\n",
    "#                 agent for agent in self.agents if agent.name.startswith(team_letter) \n",
    "#                 and agent != last_speaker and agent != self.previous_speaker\n",
    "#             ]\n",
    "\n",
    "#         self.previous_speaker = last_speaker\n",
    "\n",
    "#         if possible_next_speakers:\n",
    "#             next_speaker = random.choice(possible_next_speakers)\n",
    "#             return next_speaker\n",
    "#         else:\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Termination message detection\n",
    "def is_termination_msg(content) -> bool:\n",
    "    have_content = content.get(\"content\", None) is not None\n",
    "    if have_content and \"TERMINATE\" in content[\"content\"]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_idea = \"\"\"On planet Earth, life exists in hostile and extreme environments, and the organisms that survive there are termed extremophiles.\"\"\"\n",
    "key_idea_description = \"\"\"Extremophiles are organisms that thrive in extreme conditions on Earth, such as high temperatures, acidity, alkalinity, or pressure. These environments include locations like deep-sea hydrothermal vents, acidic hot springs, or subglacial lakes in Antarctica. The adaptability of these organisms expands our understanding of life's limits and opens up possibilities for life in extreme conditions beyond Earth.\"\"\"\n",
    "theory = \"\"\"Extremophiles and Their Environments\n",
    "Definition: Extremophiles are primarily unicellular microbes, including bacteria and archaea, that live in harsh environments​​.\n",
    "Habitats: These environments include deep-sea hydrothermal vents with high pressure and temperatures (up to 122 °C), volcanic hot springs, deep underground in mines, and in bodies of water that are highly acidic, alkaline, extremely salty, or even radioactive​​.\n",
    "The Significance of Extremophiles\n",
    "Adaptation and Survival: Extremophiles have adapted to survive in conditions that are generally detrimental to most life forms. Their existence expands our understanding of the limits and versatility of life.\n",
    "Implications for Extraterrestrial Life: The study of extremophiles has significant implications for astrobiology. The discovery of diverse microbial communities in extreme environments like subglacial lakes under the Antarctic ice sheet suggests that life might exist in similar extreme conditions elsewhere in the universe, such as on ice-covered moons like Europa​​.\n",
    "Europa: A Candidate for Extraterrestrial Life\n",
    "Characteristics of Europa: Europa, a moon of Jupiter, is a primary candidate for extraterrestrial life exploration. It has an ice-covered surface, many kilometers thick, with a possible sub-surface ocean of salty water. The surface of Europa stretches and relaxes due to tidal movements caused by its elliptical orbit around Jupiter, generating heat that could keep a sub-surface ocean in liquid state​​.\n",
    "Potential Habitability: Europa is considered a promising location for life beyond Earth due to its thin ice shell, liquid ocean, and contact with a geologically active rocky core. The moon has water and the right chemical elements, along with a potentially stable environment​​.\n",
    "Broader Understanding\n",
    "Cellular Requirements for Life: For life to exist, certain conditions are necessary, including the availability of an energy source, liquid water for biochemical reactions, and essential chemical building blocks like carbon, oxygen, nitrogen, and hydrogen. Life can thrive in stable environmental conditions within a specific range of pressure, temperature, light intensity, pH, and salinity​​.\n",
    "Challenging Traditional Concepts: Extremophiles challenge the traditional concept of habitable environments, indicating that life can exist in conditions previously thought to be uninhabitable. This expands our understanding of possible habitats for life, both on Earth and potentially in extraterrestrial settings.\n",
    "This comprehensive overview illustrates the remarkable adaptability of life and opens up new possibilities for understanding life's potential existence in extreme environments, both on Earth and beyond.\"\"\"\n",
    "student_year_level = \"Year 11\"\n",
    "subject = \"Biology\"\n",
    "question_difficulty = \"moderate\"\n",
    "question_examples_easy = [\"What is an extremophile? An extremophile is an organism that thrives in extreme environmental conditions that are typically detrimental to most life forms.\", \"Give one example of an extreme environment on Earth. One example is deep-sea hydrothermal vents.\", \"What is unique about Europa, one of Jupiter's moons? Europa has an ice-covered surface and is thought to have a sub-surface ocean of salty water.\"]\n",
    "question_examples_moderate = [\"How do extremophiles survive in high temperatures? Extremophiles in high temperatures have specialized proteins and enzymes that remain stable and functional at elevated temperatures.\", \"Why is the study of extremophiles important? Studying extremophiles helps understand the adaptability and limits of life, and it may provide insights into the possibility of life on other planets or moons.\", \"Why is Europa considered a likely place to find life beyond Earth? Europa is considered likely because of its ice shell, liquid ocean, and geologically active core, which provide conditions potentially suitable for life.\"]\n",
    "question_examples_hard = [\"Explain how extremophiles challenge the traditional concept of habitable environments. Extremophiles challenge the traditional concept by surviving in environments previously thought to be uninhabitable, expanding our understanding of where life can exist\", \"Discuss the potential implications of finding life in subglacial lakes in Antarctica for the search for extraterrestrial life. The discovery in subglacial lakes suggests that similar extreme, ice-covered environments in the solar system, such as on Europa, could also harbor life.\", \"How does the study of extremophiles contribute to our understanding of the potential for life on planets and moons outside the 'Goldilocks Zone'? The study of extremophiles indicates that life can exist in conditions not reliant on a star's warmth, suggesting that planets and moons outside the 'Goldilocks Zone' could also support life.\"]\n",
    "subject = 'biology'\n",
    "question_examples = question_examples_moderate\n",
    "other_system_instructions = \"\"\n",
    "student_interests = [\"gaming\", \"politics\", \"philosophy\"]\n",
    "student_career_goals = [\"lawyer\", \"doctor\", \"engineer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_config = {\n",
    "    \"cache_seed\": 42,\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\":config_list,\n",
    "    \"timeout\":120,\n",
    "}\n",
    "\n",
    "Human = autogen.UserProxyAgent(\n",
    "    name = \"Human_Student\",\n",
    "    system_message = \"A human student who will answer the question generated by the teacher and will be marked by the grader. The student can ask the Question Clarifier for clarification on questions posed by the teacher. If the student has finished a question (or before one is asked) they can ask questions of the theory discusser instead\",\n",
    "    code_execution_config=False\n",
    ")\n",
    "question_generator = autogen.AssistantAgent(\n",
    "    name = \"AI_Teacher\",\n",
    "    llm_config=config_list,\n",
    "    system_message=f\"\"\"You are a professional question writer for textbooks and exams. Your task is to craft a {subject} question suitable for an Australian highschool student in {student_year_level}. This question will be given to a human student to answer. Your question should be based on the following information:\n",
    "\n",
    "- This is the \"key idea\" being assessed and what your question explore: {key_idea}.\n",
    "- Ensure the question aligns with these complexities and nuances: {key_idea_description}.\n",
    "- Here is some additional theory surrounding the key idea. This information is relevant to helping you formulate the question but you do not have to strictly follow it. Take some personal liberty while still being within the same context governed by the key idea: {theory}.\n",
    "- Student Year Level - It is important you create create a question that is appropriate for this a student at this academic level: {student_year_level}.\n",
    "- Ensure the question pertains to and enriches understanding in this field: {subject}.\n",
    "- Difficulty Level - The question's complexity should be tailored to this setting: {question_difficulty}.\n",
    "\n",
    "Where applicable, embed the student's interests or career goals into the question to enhance engagement. If these elements don't directly align with the academic content, focus on the educational aspect.\n",
    "\n",
    "- Student Interests - Consider these to make the question more engaging: {student_interests}.\n",
    "- Student Career Goals - Incorporate these where relevant: {student_career_goals}.\n",
    "\n",
    "Directly pose the question without prefacing it as a 'question' or any thing else like that. The question should be formatted as if it were written in a textbook or exam, ensuring it adheres to the specified difficulty level and educational goals.\n",
    "\n",
    "Formulate a question that is academically suitable for a student at the {student_year_level} level. The question should match the specified difficulty of '{question_difficulty}'. Here are some example questions which are about the same key idea and at a similar difficulty. Do not questions directly, they are just meant to be examples of the expected difficulty:\n",
    "\n",
    "Example Questions: {question_examples}\n",
    "\n",
    "Additional Instructions: {other_system_instructions}\"\"\"\n",
    ")\n",
    "\n",
    "AI_student = autogen.AssistantAgent(\n",
    "    name = \"AI_student\",\n",
    "    llm_config=config_list,\n",
    "    system_message= f\"\"\"You are an AI Australian {student_year_level} {subject} student. Your task is to answer the question generated by the AI Teacher. Your answer should be at the sophisitcation which is appropriate to an Australian student such as yourself, answering to a depth that would be exepected from a student in {student_year_level} and nothing more. Your answer should be base on the following information:\n",
    "- Key Idea: {key_idea}. This is the primary concept that the question is exploring and the one you should focus on.\n",
    "- Detailed Description of Key Idea: {key_idea_description}. Use this information to ensure that your answer fully encompasses the nuances of the key idea.\n",
    "- Relevant Theory: {theory}. This is additional information surrounding the 'key idea'. It may or may not necessarily be directly related, use your own discretion.\n",
    "- Student Year Level: {student_year_level}. Tailor the sophistication and depth of your answer to be appropriate for a student at this academic level.\n",
    "- Additional Instructions: {other_system_instructions}. Consider these instructions to guide the style, format, or additional content of your answer.\n",
    "\n",
    "Your response should directly address the question, formulated in a clear and concise manner, suitable for the specified student year level. Avoid prefacing your response with 'Answer:' or similar qualifiers. Instead, provide a straightforward explanation or solution as one would respond in an exam or to a textbook question. Your answer will be the solution used to grade another student's response, so ensure it is accurate and complete. The Grader will use your answer as a benchmakr for the human student's response.\n",
    "\n",
    "Consider the depth of your response, ensuring that it is appropriate for the student's year level. Your answer should be sufficiently detailed to provide a comprehensive explanation, but not so detailed that it that it would be unusual for a student at a {student_year_level} academic level to provide so much depth. Your answer should be accurate and relevant to the question, avoiding unnecessary or irrelevant information.\"\"\"\n",
    ")\n",
    "\n",
    "AI_quality_control = autogen.AssistantAgent(\n",
    "    name = \"AI_Quality_Control\",\n",
    "    llm_config= config_list,\n",
    "    system_message=\"\"\"You are an expert AI who's job is to ensure ensure the information posed by the AI in this groupchat is accurate and up to standard with the prompts they have been given\"\"\"\n",
    ")\n",
    "\n",
    "AI_marker = autogen.AssistantAgent(\n",
    "    name = \"Grader\",\n",
    "    llm_config=config_list,\n",
    "    system_message = f\"\"\"\n",
    "You are an AI evaluator tasked with assessing an Australian high school student's answer to a given question, providing a grade, and offering feedback based on their performance and number of attempts. Your evaluation should focus on the accuracy and understanding of the key idea and relevant theory, appropriate to the student's academic level.\n",
    "\n",
    "Your task involves the following steps:\n",
    "\n",
    "1. **Analyzing the Student's Answer**:\n",
    "   - Analyze the student's answer against the key idea, relevant theory, and solution provided.\n",
    "   - Identify errors or misconceptions in the student's response.\n",
    "   - Use theAI student's answer as a benchmark for what is expected of {student_year_level} student.\n",
    "\n",
    "2. **Providing Feedback**:\n",
    "   - Based on the score and the number of attempts, decide on the type of feedback to provide.\n",
    "   - If the score is high (80-100) and the answer mostly correct, congratulate the student and point out any minor misconceptions or areas for improvement.\n",
    "   - If the score is lower and this is not the third attempt, encourage the student to try again, providing guidance on what aspects need more focus.\n",
    "   - If this is the third attempt or the student has significant misconceptions, explain the correct answer and the reasoning behind it, being considerate and supportive.\n",
    "\n",
    "3. **Giving Grade**\n",
    "    - AFTER you have analyzed the students answer and provided feedback, Assign a score out of 100, considering the student's academic level and the completeness of their understanding of the key concepts.\n",
    "\n",
    "3. **Next Steps**:\n",
    "   - Ask the student if they would like to move on to the next question, discuss additional theory about the current question, or clarify any misunderstandings about the question itself.\n",
    "\n",
    "Construct a response that includes the score, feedback, and next steps, formatted appropriately.\n",
    "\n",
    "Here is some context to guide your evaluation and feedback:\n",
    "\n",
    "- Key Idea: {key_idea}\n",
    "- Description of Key Idea: {key_idea_description}\n",
    "- Relevant Theory: {theory}\n",
    "- Student Year Level: {student_year_level}\n",
    "- Additional Instructions: {other_system_instructions}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "discuss_theory = autogen.AssistantAgent(\n",
    "    name = \"Theory_Discusser\",\n",
    "    llm_config=config_list,\n",
    "    system_message = f\"\"\" You are an AI teaching assistant who discusses the theory surrounding a question the student has just answered You now tasked with answering questions about the theory related to the question. Your answers should be appropriate for the student's academic level, avoiding the use of sophisticated jargon unless directly asked. If relevant, tailor your discussion of the theroy around the question that was provided to the student, the student's answer(s) and the feedback you have provided them.\n",
    "\n",
    "At the end of every message you send to the student, ask them if they would like to move on to the next question or if they would like to keep discussing more theory. You are the theory discusser. When prompted by the human student you should discuss the theory the student wants to go over.You are only to answer the human student's questions if AND ONLY IF the student isn't currently answering a question. You are only to respond to the human student if they have already answered the question correctly or if the Grader has told decided that the human student they are incorrect and already provided the soltion. It is very important you do not reveal the answer to the student while they still ahven't answered the question.\n",
    "\n",
    "Here is some relevant infromation reagrding the question the student was asked to answer: \n",
    "\n",
    "- Key Idea - Assess/Evaluate the student's understanding and expression of this concept: {key_idea}.\n",
    "- Description of Key Idea - Use this information to ensure that your evaluation fully encompasses the nuances of the key idea: {key_idea_description}. \n",
    "- Relevant Theory - Look for key differences/Assess how well the student's answer reflects an understanding of this theory: {theory}.\n",
    "- Student Year Level - Evaluate/Consider the academic level of the student, focusing on their grasp of the concept rather than the depth of the response: {student_year_level}.\n",
    "- Additional Instructions  Consider these instructions to guide the style, format, or additional content of your answer: {other_system_instructions}.\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "clarifying_AI = autogen.AssistantAgent(\n",
    "    name = \"Question_Clarifier\",\n",
    "    llm_config=config_list,\n",
    "    system_message = f\"\"\"You are an AI assistant that has been helping An Australian high school student in {student_year_level} learn by asking questions in the field of {subject}. They are now asking to clarify something about a question you have just asked but which they have yet to answer correctly. Additional theory surrounding a key idea which will be provided, along side some theory about the topic which is appropriate for their academic level. You are now tasked with answering questions about the theory related to the question. Your answers should be appropriate for the student's academic level, avoiding the use of sophisticated jargon unless directly asked. If relevant, tailor your discussion of the theroy around the question that was provided to the student, the student's answer(s) and the feedback you have provided them\n",
    "    \n",
    "It is critical you do not reveal the any solutions as the student is still attempting to answer the question and may attempt to do so again. As an example, if the question you have asked them is \"what does the mitochondria do\" and the student says \"what's the difference between the the mitochondria and the golgi apparatus\" then you should try to avoid answering their clarification question in a way that might reveal what the mitochondria does, for example by saying \"well the golgi apparatus does not generate most of the chemical energy needed to power the cell's biochemical reactions.\" as this would reveal the answer to the student. Do not strictly follow this format, it is just an example but it captures the essence of what you should do.\n",
    "\n",
    "After you provide your clarification to the student, ask the student if they are ready to answer the question or if they need more clarification.\"\"\")\n",
    "\n",
    "# initial_message = \"\"\"\n",
    "# Work together to create a question for a human student. The general flow of the conversation should be as follows:\n",
    "\n",
    "# 1. Teacher makes a question based on the provided information.\n",
    "# The question should be sent to the human student for them to answer.\n",
    "# 2. While the human student is answering the question, AI student answers the question before the human student\n",
    "# 3. AI quality control checks that the answer the AI student gave is correct and of good quality\n",
    "# 4. wait for the Human student to provide an answer\n",
    "# 5. Grader will take the human answer and grade it\n",
    "# 6. Depending on the result of the grade, the Human student will either be congratulated or asked to try again if they got it wrong (less then 80%) or they the grader will tell the student they are incorrect and provide the solution if they got it wrong three times.\n",
    "# 7. If the student wants to discuss theory with the theory discusser, they can do so.\n",
    "# 8. If the students want to move on to the next question, then the teacher will generate a question and the cycle will repeat.\n",
    "# If the student has not answered the question correctly yet, or if the student has not answered it incorrectly 3 times yet, then if they ask for clarification on the question they are asnwering then Question Clarifier will help clarify questions\n",
    "# \"\"\"\n",
    "\n",
    "groupchat = autogen.GroupChat(agents = [Human, question_generator, AI_student, AI_quality_control, AI_marker, discuss_theory, clarifying_AI],messages=[], max_round=50)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mHuman_Student\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "Work together to create a question for a human student. The general flow of the conversation should be as follows:\n",
      "\n",
      "1. Teacher makes a question based on the provided information.\n",
      "The question should be sent to the human student for them to answer.\n",
      "2. While the human student is answering the question, AI student answers the question before the human student\n",
      "3. AI quality control checks that the answer the AI student gave is correct and of good quality\n",
      "4. wait for the Human student to provide an answer\n",
      "5. Grader will take the human answer and grade it\n",
      "6. Depending on the result of the grade, the Human student will either be congratulated or asked to try again if they got it wrong (less then 80%) or they the grader will tell the student they are incorrect and provide the solution if they got it wrong three times.\n",
      "7. If the student wants to discuss theory with the theory discusser, they can do so.\n",
      "8. If the students want to move on to the next question, then the teacher will generate a question and the cycle will repeat.\n",
      "If the student has not answered the question correctly yet, or if the student has not answered it incorrectly 3 times yet, then if they ask for clarification on the question they are asnwering then Question Clarifier will help clarify questions\n",
      "If at any point the student says they don't know the answer then Question_Clarifer should step in to offer to clarify the question\"\n",
      "If the student says something unreleated to the question you have just asked them then the the agent who has most recently spoken to them should tespond to the student by saying that they are not answering the question and that they should try to answer the question instead.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 5049 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/andrew/Desktop/Everdawn/Eduflow AI/EduFlow/Auto/main.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Human\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     manager,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mWork together to create a question for a human student. The general flow of the conversation should be as follows:\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m1. Teacher makes a question based on the provided information.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mThe question should be sent to the human student for them to answer.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m2. While the human student is answering the question, AI student answers the question before the human student\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m3. AI quality control checks that the answer the AI student gave is correct and of good quality\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m4. wait for the Human student to provide an answer\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m5. Grader will take the human answer and grade it\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m6. Depending on the result of the grade, the Human student will either be congratulated or asked to try again if they got it wrong (less then 80\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m) or they the grader will tell the student they are incorrect and provide the solution if they got it wrong three times.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m7. If the student wants to discuss theory with the theory discusser, they can do so.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m8. If the students want to move on to the next question, then the teacher will generate a question and the cycle will repeat.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mIf the student has not answered the question correctly yet, or if the student has not answered it incorrectly 3 times yet, then if they ask for clarification on the question they are asnwering then Question Clarifier will help clarify questions\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mIf at any point the student says they don\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt know the answer then Question_Clarifer should step in to offer to clarify the question\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mIf the student says something unreleated to the question you have just asked them then the the agent who has most recently spoken to them should tespond to the student by saying that they are not answering the question and that they should try to answer the question instead.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andrew/Desktop/Everdawn/Eduflow%20AI/EduFlow/Auto/main.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:550\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 550\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:348\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    346\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 348\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    349\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    351\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:481\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:906\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 906\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    908\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/autogen/agentchat/groupchat.py:269\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[39m# select the next speaker\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     speaker \u001b[39m=\u001b[39m groupchat\u001b[39m.\u001b[39;49mselect_speaker(speaker, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    270\u001b[0m     \u001b[39m# let the speaker speak\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     reply \u001b[39m=\u001b[39m speaker\u001b[39m.\u001b[39mgenerate_reply(sender\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/autogen/agentchat/groupchat.py:158\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m# auto speaker selection\u001b[39;00m\n\u001b[1;32m    157\u001b[0m selector\u001b[39m.\u001b[39mupdate_system_message(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_speaker_msg(agents))\n\u001b[0;32m--> 158\u001b[0m final, name \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mgenerate_oai_reply(\n\u001b[1;32m    159\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessages\n\u001b[1;32m    160\u001b[0m     \u001b[39m+\u001b[39;49m [\n\u001b[1;32m    161\u001b[0m         {\n\u001b[1;32m    162\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    163\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRead the above conversation. Then select the next role from \u001b[39;49m\u001b[39m{\u001b[39;49;00m[agent\u001b[39m.\u001b[39;49mname\u001b[39m \u001b[39;49m\u001b[39mfor\u001b[39;49;00m\u001b[39m \u001b[39;49magent\u001b[39m \u001b[39;49m\u001b[39min\u001b[39;49;00m\u001b[39m \u001b[39;49magents]\u001b[39m}\u001b[39;49;00m\u001b[39m to play. Only return the role.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    164\u001b[0m         }\n\u001b[1;32m    165\u001b[0m     ]\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m final:\n\u001b[1;32m    168\u001b[0m     \u001b[39m# the LLM client is None, thus no reply is generated. Use round robin instead.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_agent(last_speaker, agents)\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:625\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    622\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    624\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    626\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages\n\u001b[1;32m    627\u001b[0m )\n\u001b[1;32m    628\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, client\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/autogen/oai/client.py:247\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[39mcontinue\u001b[39;00m  \u001b[39m# filter is not passed; try the next config\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_completions_create(client, params)\n\u001b[1;32m    248\u001b[0m \u001b[39mexcept\u001b[39;00m APIError:\n\u001b[1;32m    249\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m failed\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/autogen/oai/client.py:327\u001b[0m, in \u001b[0;36mOpenAIWrapper._completions_create\u001b[0;34m(self, client, params)\u001b[0m\n\u001b[1;32m    325\u001b[0m     params \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    326\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     response \u001b[39m=\u001b[39m completions\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    328\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    599\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    600\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    601\u001b[0m             {\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    620\u001b[0m             },\n\u001b[1;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    622\u001b[0m         ),\n\u001b[1;32m    623\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    627\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1050\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1051\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1060\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1061\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    843\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    844\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    845\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    846\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    847\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    848\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Everdawn/Eduflow AI/EduFlow/.venv/lib/python3.10/site-packages/openai/_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    887\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 5049 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "Human.initiate_chat(\n",
    "    manager,\n",
    "    message=\"\"\"\n",
    "Work together to create a question for a human student. The general flow of the conversation should be as follows:\n",
    "\n",
    "1. Teacher makes a question based on the provided information.\n",
    "The question should be sent to the human student for them to answer.\n",
    "2. While the human student is answering the question, AI student answers the question before the human student\n",
    "3. AI quality control checks that the answer the AI student gave is correct and of good quality\n",
    "4. wait for the Human student to provide an answer\n",
    "5. Grader will take the human answer and grade it\n",
    "6. Depending on the result of the grade, the Human student will either be congratulated or asked to try again if they got it wrong (less then 80%) or they the grader will tell the student they are incorrect and provide the solution if they got it wrong three times.\n",
    "7. If the student wants to discuss theory with the theory discusser, they can do so.\n",
    "8. If the students want to move on to the next question, then the teacher will generate a question and the cycle will repeat.\n",
    "If the student has not answered the question correctly yet, or if the student has not answered it incorrectly 3 times yet, then if they ask for clarification on the question they are asnwering then Question Clarifier will help clarify questions\n",
    "If at any point the student says they don't know the answer then Question_Clarifer should step in to offer to clarify the question\"\n",
    "If the student says something unreleated to the question you have just asked them then the the agent who has most recently spoken to them should tespond to the student by saying that they are not answering the question and that they should try to answer the question instead.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
